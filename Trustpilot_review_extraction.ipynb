{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trustpilot_review_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Trustpilot Review Scraper\n",
    "\n",
    "This script allows you to scrape Trustpilot reviews for a given Housing Association or company. \n",
    "It can handle URLs in various formats, including those with or without the 'page' parameter. \n",
    "The scraper fetches review data from multiple pages, if available, and stores the results in a CSV file.\n",
    "\n",
    "Usage:\n",
    "1. Set the base URL for the company or Housing Association you want to scrape.\n",
    "2. The script automatically generates URLs for all available pages (if the 'page' parameter is included) \n",
    "   or just scrapes the first page (if no 'page' parameter is present).\n",
    "3. Reviews are extracted, including user names, user links, publication dates, review bodies, and ratings.\n",
    "4. The scraped data is saved to a CSV file for easy analysis.\n",
    "\n",
    "Example:\n",
    "- For a URL with 'page' and 'sort' parameters:\n",
    "  url = 'https://uk.trustpilot.com/review/www.jigsawhomes.org.uk?page=8&sort=recency'\n",
    "\n",
    "- For a URL without the 'page' parameter (only one page):\n",
    "  url = 'https://uk.trustpilot.com/review/placesforpeople.co.uk'\n",
    "\n",
    "- For a URL with 'sort' only (but no 'page'):\n",
    "  url = 'https://uk.trustpilot.com/review/www.myclarionhousing.com?sort=recency'\n",
    "\n",
    "To save the results to a CSV file, use the `save_reviews_to_csv` method with the desired file path.\n",
    "\n",
    "Example:\n",
    "  output_path = \"YOURpath/yourhousinggroup.csv\"\n",
    "  scraper.save_reviews_to_csv(output_path)\n",
    "\n",
    "Requirements:\n",
    "- `requests`\n",
    "- `beautifulsoup4`\n",
    "- `pandas`\n",
    "\n",
    "Ensure that these libraries are installed before running the script. You can install them using:\n",
    "  pip install requests beautifulsoup4 pandas\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "\n",
    "class TrustpilotScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.full_url_list = self.generate_url_list()\n",
    "\n",
    "    def generate_url_list(self):\n",
    "        \"\"\"Generate a list of URLs for all available pages. Handles URLs with or without the 'page' parameter.\"\"\"\n",
    "        base_url, query_params = self.url.split(\"?\") if \"?\" in self.url else (self.url, '')\n",
    "        \n",
    "        # If the URL doesn't contain a page parameter, assume just the first page\n",
    "        if \"page=\" in query_params:\n",
    "            page_part, sort_part = query_params.split(\"&\")\n",
    "            current_page = int(page_part.split(\"=\")[1])\n",
    "            return [\n",
    "                f\"{base_url}?page={i}&{sort_part}\" for i in range(1, current_page + 1)\n",
    "            ]\n",
    "        else:\n",
    "            # If no page parameter, assume just one page\n",
    "            return [base_url]\n",
    "\n",
    "    def fetch_reviews(self, url):\n",
    "        \"\"\"Fetch review data from the provided URL.\"\"\"\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        html_string = str(soup.find_all(re.compile('script'))[6])  # The JSON is typically in the 6th script tag\n",
    "        return self.parse_json_data(html_string)\n",
    "\n",
    "    def parse_json_data(self, html_string):\n",
    "        \"\"\"Parse the JSON data embedded in the script tag.\"\"\"\n",
    "        class MyHTMLParser(HTMLParser):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.data = []\n",
    "\n",
    "            def handle_data(self, data):\n",
    "                self.data.append(data)\n",
    "\n",
    "        parser = MyHTMLParser()\n",
    "        parser.feed(html_string)\n",
    "        json_output = json.dumps({\"TheData\": parser.data[0]})\n",
    "        return json.loads(json_output)\n",
    "\n",
    "    def extract_review_data(self, json_data):\n",
    "        \"\"\"Extract relevant review data from the parsed JSON.\"\"\"\n",
    "        reviews_data = list(json_data.items())[0][1]\n",
    "        \n",
    "        user_links = re.findall(r'https://uk.trustpilot.com/users/[0-9a-z/]*', reviews_data)\n",
    "        user_names = self.extract_user_names(reviews_data)\n",
    "        dates = self.extract_dates(reviews_data)\n",
    "        review_bodies = self.extract_review_bodies(reviews_data)\n",
    "        ratings = self.extract_ratings(reviews_data)\n",
    "        \n",
    "        return user_links, user_names, dates, review_bodies, ratings\n",
    "\n",
    "    def extract_user_names(self, reviews_data):\n",
    "        \"\"\"Extract user names from the review data.\"\"\"\n",
    "        name_matches = re.finditer(\"name\", reviews_data)\n",
    "        names = [\n",
    "            reviews_data[(match.span()[1] + 3):(match.span()[1] + 30)]\n",
    "            for match in name_matches\n",
    "        ]\n",
    "        return [name.split('\"')[0] for name in names]  # Cleanup names\n",
    "\n",
    "    def extract_dates(self, reviews_data):\n",
    "        \"\"\"Extract publication dates from the review data.\"\"\"\n",
    "        date_matches = re.finditer(\"datePublished\", reviews_data)\n",
    "        return [\n",
    "            reviews_data[(match.span()[1] + 3):(match.span()[1] + 27)]\n",
    "            for match in date_matches\n",
    "        ]\n",
    "\n",
    "    def extract_review_bodies(self, reviews_data):\n",
    "        \"\"\"Extract review bodies from the review data.\"\"\"\n",
    "        review_body_matches = zip(\n",
    "            re.finditer(\"reviewBody\", reviews_data),\n",
    "            re.finditer(\"reviewRating\", reviews_data),\n",
    "        )\n",
    "        return [\n",
    "            reviews_data[(match1.span()[1] + 2):(match2.span()[0] - 2)]\n",
    "            for match1, match2 in review_body_matches\n",
    "        ]\n",
    "\n",
    "    def extract_ratings(self, reviews_data):\n",
    "        \"\"\"Extract rating values from the review data.\"\"\"\n",
    "        rating_matches = re.finditer(\"ratingValue\", reviews_data)\n",
    "        return [\n",
    "            reviews_data[(match.span()[1] + 3):(match.span()[1] + 4)]\n",
    "            for match in rating_matches\n",
    "        ]\n",
    "\n",
    "    def extract_housing_association(self, url):\n",
    "        \"\"\"Extract housing association name from the URL.\"\"\"\n",
    "        result = url.partition(\"https://uk.trustpilot.com/review/\")[2].partition(\"?\")\n",
    "        ha_0 = result[0].split(\".\")\n",
    "        return ha_0[1] if ha_0[0] == 'www' else ha_0[0]\n",
    "\n",
    "    def scrape_reviews(self):\n",
    "        \"\"\"Scrape reviews from all the pages and save to a DataFrame.\"\"\"\n",
    "        all_reviews = []\n",
    "\n",
    "        for url in self.full_url_list:\n",
    "            json_data = self.fetch_reviews(url)\n",
    "            user_links, user_names, dates, review_bodies, ratings = self.extract_review_data(json_data)\n",
    "            housing_association = [self.extract_housing_association(url)] * len(user_links)\n",
    "            data_source = [url] * len(user_links)\n",
    "\n",
    "            all_reviews.extend(zip(housing_association, data_source, user_names, user_links, dates, review_bodies, ratings))\n",
    "\n",
    "        # Create a DataFrame and return it\n",
    "        df = pd.DataFrame(all_reviews, columns=['HousingAssociation', 'DataSource', 'User', 'UserURL', 'DatePublished', 'ReviewBody', 'RatingValue'])\n",
    "        return df\n",
    "\n",
    "    def save_reviews_to_csv(self, output_path):\n",
    "        \"\"\"Save the scraped reviews to a CSV file.\"\"\"\n",
    "        df = self.scrape_reviews()\n",
    "        df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "url = 'https://uk.trustpilot.com/review/placesforpeople.co.uk'  # URL without 'page' parameter\n",
    "scraper = TrustpilotScraper(url)\n",
    "output_path = \"YOURpath/placesforpeople.csv\"\n",
    "scraper.save_reviews_to_csv(output_path)\n",
    "\n",
    "url = 'https://uk.trustpilot.com/review/www.myclarionhousing.com?sort=recency'  # URL with 'sort' but no 'page'\n",
    "scraper = TrustpilotScraper(url)\n",
    "output_path = \"YOURpath/myclarionhousing.csv\"\n",
    "scraper.save_reviews_to_csv(output_path)\n",
    "\n",
    "url = 'https://uk.trustpilot.com/review/www.jigsawhomes.org.uk?page=8&sort=recency'  # URL with 'page' and 'sort'\n",
    "scraper = TrustpilotScraper(url)\n",
    "output_path = \"YOURpath/jigsawhomes.csv\"\n",
    "scraper.save_reviews_to_csv(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
